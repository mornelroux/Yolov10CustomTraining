{"cells":[{"cell_type":"markdown","metadata":{"id":"kA3YK_kpUrKN"},"source":["# YOLOv10 Custom training\n","Jupyter notebook used to train custom model on Google CoLab for specific application\n"]},{"cell_type":"code","source":["from google.colab import drive\n","import os\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","DRIVE_PATH = '/content/drive/MyDrive/'\n","\n","#Change directory to Google Drive Direcotory\n","os.chdir(f'{DRIVE_PATH}')\n","\n","#Set as Home directory\n","HOME = os.getcwd()\n","print(HOME)"],"metadata":{"id":"WlO77xa0BrK5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724588237079,"user_tz":-120,"elapsed":4673,"user":{"displayName":"Morne Le Roux","userId":"03856785595107178262"}},"outputId":"1fb458ec-02af-493b-88f2-33cb71dcbe99"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive\n"]}]},{"cell_type":"code","execution_count":15,"metadata":{"id":"doBsK2Vghq42","executionInfo":{"status":"ok","timestamp":1724588237080,"user_tz":-120,"elapsed":9,"user":{"displayName":"Morne Le Roux","userId":"03856785595107178262"}}},"outputs":[],"source":["#Define Session\n","SESSION_NAME    = 'OpenImageTest'\n","MODEL_PATH      = f'{HOME}/weights/yolov10n.pt'\n","\n","#Dataset Variablesh\n","classes = ['Horse','Sheep','Cattle']\n","\n","#Training Variables\n","EPOCHS = 30\n","\n","#Validation Variables\n","number_mapping = {\n","    '0': '17',      # Dataset ID : COCO ID\n","    '1': '18',\n","    '2': '19',\n","}"]},{"cell_type":"markdown","source":["#Add Dataset Paths"],"metadata":{"id":"q8aUUbOWECK_"}},{"cell_type":"code","source":["DATASETS_DIR          = f'{HOME}/datasets'\n","DATASET_DIR           = f'{DATASETS_DIR}/OpenImages-LiveStock'\n","DATASET_YAML_PATH     = f'{DATASET_DIR}/dataset.yaml'"],"metadata":{"id":"zp-M3ouBEBUN","executionInfo":{"status":"ok","timestamp":1724588237080,"user_tz":-120,"elapsed":8,"user":{"displayName":"Morne Le Roux","userId":"03856785595107178262"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7v5o7jVTiAmg"},"source":["## Enable GPU acceleration\n","Navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`. This will ensure your notebook uses a GPU, which will significantly speed up model training times."]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y8cDtxLIBHgQ","outputId":"15ca6449-05d4-452a-d23a-5daad7073bbf","executionInfo":{"status":"ok","timestamp":1724588237080,"user_tz":-120,"elapsed":7,"user":{"displayName":"Morne Le Roux","userId":"03856785595107178262"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Aug 25 12:17:16 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   77C    P0              35W /  70W |    901MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["#Check if GPU is active\n","!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"3C3EO_2zNChu"},"source":["## Install YOLOv10"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tdSMcABDNKW-","outputId":"cb551ec6-277b-46cc-c57a-b0931984a204","executionInfo":{"status":"ok","timestamp":1724588248855,"user_tz":-120,"elapsed":11779,"user":{"displayName":"Morne Le Roux","userId":"03856785595107178262"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","total 55M\n","-rw------- 1 root root 11M May 26 15:54 yolov10n.pt\n","-rw------- 1 root root 11M May 26 15:54 yolov10n.pt.1\n","-rw------- 1 root root 11M May 26 15:54 yolov10n.pt.2\n","-rw------- 1 root root 11M May 26 15:54 yolov10n.pt.3\n","-rw------- 1 root root 11M May 26 15:54 yolov10n.pt.4\n"]}],"source":["# Clone YOLOv10 from Github\n","!pip install -q git+https://github.com/THU-MIG/yolov10.git\n","\n","# Download pretrained weights\n","!mkdir -p {HOME}/weights\n","!wget -P {HOME}/weights -q https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10n.pt\n","# !wget -P {HOME}/weights -q https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10s.pt\n","# !wget -P {HOME}/weights -q https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10m.pt\n","# !wget -P {HOME}/weights -q https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10b.pt\n","# !wget -P {HOME}/weights -q https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10x.pt\n","# !wget -P {HOME}/weights -q https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10l.pt\n","!ls -lh {HOME}/weights"]},{"cell_type":"markdown","metadata":{"id":"YUjFBKKqXa-u"},"source":["## Custom Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D2YkphuiaE7_","outputId":"ef170f55-0580-4069-b9e6-c3d77c679d77"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive\n","New https://pypi.org/project/ultralytics/8.2.81 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n","Ultralytics YOLOv8.1.34 ðŸš€ Python-3.10.12 torch-2.3.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/content/drive/MyDrive/weights/yolov10n.pt, data=/content/drive/MyDrive/datasets/OpenImages-LiveStock/dataset.yaml, epochs=30, time=None, patience=100, batch=-1, imgsz=640, save=True, save_period=-1, val_period=1, cache=False, device=None, workers=8, project=/content/drive/MyDrive/runs/OpenImageTest, name=OpenImageTest-20240825-12:17, exist_ok=False, pretrained=True, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=2.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=/content/drive/MyDrive/runs/OpenImageTest/OpenImageTest-20240825-12:17\n","Overriding model.yaml nc=80 with nc=4\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n","  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n","  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n","  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n","  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n","  5                  -1  1      9856  ultralytics.nn.modules.block.SCDown          [64, 128, 3, 2]               \n","  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n","  7                  -1  1     36096  ultralytics.nn.modules.block.SCDown          [128, 256, 3, 2]              \n","  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n","  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n"," 10                  -1  1    249728  ultralytics.nn.modules.block.PSA             [256, 256]                    \n"," 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 13                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n"," 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 16                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n"," 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n"," 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 19                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n"," 20                  -1  1     18048  ultralytics.nn.modules.block.SCDown          [128, 128, 3, 2]              \n"," 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 22                  -1  1    282624  ultralytics.nn.modules.block.C2fCIB          [384, 256, 1, True, True]     \n"," 23        [16, 19, 22]  1    862888  ultralytics.nn.modules.head.v10Detect        [4, [64, 128, 256]]           \n","YOLOv10n summary: 385 layers, 2708600 parameters, 2708584 gradients, 8.4 GFLOPs\n","\n","Transferred 493/595 items from pretrained weights\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /content/drive/MyDrive/runs/OpenImageTest/OpenImageTest-20240825-12:17', view at http://localhost:6006/\n","Freezing layer 'model.23.dfl.conv.weight'\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","\u001b[34m\u001b[1mAutoBatch: \u001b[0mComputing optimal batch size for imgsz=640\n","\u001b[34m\u001b[1mAutoBatch: \u001b[0mCUDA:0 (Tesla T4) 14.75G total, 0.62G reserved, 0.15G allocated, 13.98G free\n","      Params      GFLOPs  GPU_mem (GB)  forward (ms) backward (ms)                   input                  output\n","     2708600         8.4         0.810         78.67           nan        (1, 3, 640, 640)                    list\n","     2708600        16.8         0.778         66.78           nan        (2, 3, 640, 640)                    list\n","     2708600        33.6         1.380         62.95           nan        (4, 3, 640, 640)                    list\n","     2708600        67.2         2.678         73.98           nan        (8, 3, 640, 640)                    list\n","     2708600       134.4         5.039            60           nan       (16, 3, 640, 640)                    list\n","\u001b[34m\u001b[1mAutoBatch: \u001b[0mUsing batch-size 27 for CUDA:0 9.01G/14.75G (61%) âœ…\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/datasets/OpenImages-LiveStock/labels/train.cache... 1000 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/albumentations/core/composition.py:161: UserWarning: Got processor for bboxes, but no transform to process it.\n","  self._set_keys()\n","/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/datasets/OpenImages-LiveStock/labels/val.cache... 200 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<?, ?it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Plotting labels to /content/drive/MyDrive/runs/OpenImageTest/OpenImageTest-20240825-12:17/labels.jpg... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 95 weight(decay=0.0), 108 weight(decay=0.000421875), 107 bias(decay=0.0)\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n","Image sizes 640 train, 640 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1m/content/drive/MyDrive/runs/OpenImageTest/OpenImageTest-20240825-12:17\u001b[0m\n","Starting training for 30 epochs...\n","\n","      Epoch    GPU_mem     box_om     cls_om     dfl_om     box_oo     cls_oo     dfl_oo  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       1/30      7.41G      1.484      2.932      1.469      1.405      4.265      1.319          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:42<00:00,  1.12s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:05<00:00,  1.46s/it]"]},{"output_type":"stream","name":"stdout","text":["                   all        200        540    0.00225      0.807    0.00734    0.00405\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem     box_om     cls_om     dfl_om     box_oo     cls_oo     dfl_oo  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       2/30      6.41G      1.653      2.154      1.575      1.597      3.571      1.403          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:35<00:00,  1.08it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.05s/it]"]},{"output_type":"stream","name":"stdout","text":["                   all        200        540    0.00313      0.859     0.0265     0.0112\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem     box_om     cls_om     dfl_om     box_oo     cls_oo     dfl_oo  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       3/30      5.62G      1.693      2.105      1.614      1.659      3.353      1.445         16        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:36<00:00,  1.04it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.18it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all        200        540      0.758     0.0115    0.00346    0.00142\n","\n","      Epoch    GPU_mem     box_om     cls_om     dfl_om     box_oo     cls_oo     dfl_oo  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       4/30      6.12G      1.711      2.116      1.645      1.732      3.171      1.493          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:40<00:00,  1.06s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.24it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        200        540      0.528     0.0503     0.0264    0.00831\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem     box_om     cls_om     dfl_om     box_oo     cls_oo     dfl_oo  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       5/30       5.7G      1.719      2.105      1.664      1.717      3.014      1.508          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:37<00:00,  1.00it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:05<00:00,  1.42s/it]"]},{"output_type":"stream","name":"stdout","text":["                   all        200        540      0.796     0.0364     0.0293     0.0112\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem     box_om     cls_om     dfl_om     box_oo     cls_oo     dfl_oo  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       6/30      5.79G      1.736      2.063      1.683      1.769      2.806      1.548          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:36<00:00,  1.04it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.10s/it]\n"]},{"output_type":"stream","name":"stdout","text":["                   all        200        540      0.791     0.0517     0.0427     0.0153\n","\n","      Epoch    GPU_mem     box_om     cls_om     dfl_om     box_oo     cls_oo     dfl_oo  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       7/30       6.2G      1.719      1.994      1.658       1.73      2.672      1.514          9        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:35<00:00,  1.06it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.08it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        200        540      0.551      0.119     0.0358     0.0147\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem     box_om     cls_om     dfl_om     box_oo     cls_oo     dfl_oo  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       8/30      6.33G      1.707      1.927       1.62      1.739      2.537      1.483          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:37<00:00,  1.01it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.18it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        200        540      0.791     0.0374     0.0255     0.0109\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem     box_om     cls_om     dfl_om     box_oo     cls_oo     dfl_oo  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       9/30      6.01G      1.675      1.845      1.647      1.711       2.42      1.539          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:40<00:00,  1.06s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.23it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        200        540      0.813     0.0651     0.0764     0.0444\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem     box_om     cls_om     dfl_om     box_oo     cls_oo     dfl_oo  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      10/30      5.53G      1.605      1.792      1.568      1.668      2.328      1.461          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:38<00:00,  1.01s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.18s/it]"]},{"output_type":"stream","name":"stdout","text":["                   all        200        540       0.61     0.0888     0.0741     0.0255\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem     box_om     cls_om     dfl_om     box_oo     cls_oo     dfl_oo  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      11/30       6.3G      1.633       1.81      1.564      1.679      2.319      1.445         50        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:35<00:00,  1.07it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:05<00:00,  1.34s/it]\n"]},{"output_type":"stream","name":"stdout","text":["                   all        200        540      0.413      0.134     0.0683     0.0298\n","\n","      Epoch    GPU_mem     box_om     cls_om     dfl_om     box_oo     cls_oo     dfl_oo  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      12/30      5.85G      1.601      1.785       1.55      1.653      2.292      1.445          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:34<00:00,  1.09it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.00s/it]"]},{"output_type":"stream","name":"stdout","text":["                   all        200        540      0.334      0.157      0.052     0.0209\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem     box_om     cls_om     dfl_om     box_oo     cls_oo     dfl_oo  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      13/30      5.92G      1.543       1.73      1.537      1.639      2.212      1.422          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:37<00:00,  1.03it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.21it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        200        540      0.357     0.0999     0.0648     0.0279\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem     box_om     cls_om     dfl_om     box_oo     cls_oo     dfl_oo  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      14/30         6G       1.56      1.685      1.525      1.632      2.142      1.428         10        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:39<00:00,  1.05s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.25it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        200        540       0.62      0.097     0.0652     0.0277\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem     box_om     cls_om     dfl_om     box_oo     cls_oo     dfl_oo  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      15/30      6.44G      1.523      1.724      1.541      1.597      2.152      1.433          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:38<00:00,  1.02s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:05<00:00,  1.37s/it]"]},{"output_type":"stream","name":"stdout","text":["                   all        200        540       0.34      0.149     0.0677     0.0307\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem     box_om     cls_om     dfl_om     box_oo     cls_oo     dfl_oo  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      16/30      6.74G      1.522      1.601      1.494      1.587      2.043        1.4          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:34<00:00,  1.09it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:05<00:00,  1.45s/it]\n"]},{"output_type":"stream","name":"stdout","text":["                   all        200        540      0.321      0.127      0.093     0.0461\n","\n","      Epoch    GPU_mem     box_om     cls_om     dfl_om     box_oo     cls_oo     dfl_oo  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      17/30      6.04G      1.504      1.733      1.518      1.581      2.125      1.422          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:35<00:00,  1.08it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.12s/it]"]},{"output_type":"stream","name":"stdout","text":["                   all        200        540      0.388      0.192      0.177      0.103\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem     box_om     cls_om     dfl_om     box_oo     cls_oo     dfl_oo  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      18/30      6.14G      1.493      1.594      1.482      1.583      2.006      1.413          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:35<00:00,  1.07it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.19it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        200        540      0.448      0.177      0.147     0.0634\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem     box_om     cls_om     dfl_om     box_oo     cls_oo     dfl_oo  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      19/30       6.3G      1.461      1.529      1.477       1.55       1.95      1.398          6        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:39<00:00,  1.03s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.19it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        200        540      0.444      0.167      0.146     0.0812\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem     box_om     cls_om     dfl_om     box_oo     cls_oo     dfl_oo  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      20/30      6.77G      1.462      1.514      1.471      1.546      1.909      1.385          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:39<00:00,  1.04s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.09it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        200        540      0.796       0.12      0.164     0.0769\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Closing dataloader mosaic\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/albumentations/core/composition.py:161: UserWarning: Got processor for bboxes, but no transform to process it.\n","  self._set_keys()\n","/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem     box_om     cls_om     dfl_om     box_oo     cls_oo     dfl_oo  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      21/30      6.11G      1.496      1.585      1.489      1.587      1.964      1.418          6        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:42<00:00,  1.13s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.00it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        200        540       0.44      0.164      0.132     0.0735\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem     box_om     cls_om     dfl_om     box_oo     cls_oo     dfl_oo  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      22/30      5.85G       1.49      1.522      1.477      1.567      1.898       1.41         37        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:33<00:00,  1.14it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:05<00:00,  1.37s/it]\n"]},{"output_type":"stream","name":"stdout","text":["                   all        200        540      0.405       0.13     0.0986     0.0496\n","\n","      Epoch    GPU_mem     box_om     cls_om     dfl_om     box_oo     cls_oo     dfl_oo  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      23/30      6.13G      1.477      1.488      1.471      1.574      1.846      1.409          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:33<00:00,  1.13it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.19it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        200        540      0.678      0.162      0.138     0.0755\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem     box_om     cls_om     dfl_om     box_oo     cls_oo     dfl_oo  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      24/30       6.3G      1.409      1.413      1.438      1.508      1.764      1.378         12        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:37<00:00,  1.02it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.21it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        200        540      0.462       0.15      0.115     0.0626\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem     box_om     cls_om     dfl_om     box_oo     cls_oo     dfl_oo  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      25/30      6.12G      1.371      1.413      1.403      1.484      1.729      1.355          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:34<00:00,  1.10it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:05<00:00,  1.27s/it]\n"]},{"output_type":"stream","name":"stdout","text":["                   all        200        540       0.34      0.179      0.106     0.0594\n","\n","      Epoch    GPU_mem     box_om     cls_om     dfl_om     box_oo     cls_oo     dfl_oo  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      26/30      6.13G      1.383      1.455      1.429       1.48      1.772      1.352          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:33<00:00,  1.13it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.23it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        200        540      0.714      0.136      0.119     0.0599\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem     box_om     cls_om     dfl_om     box_oo     cls_oo     dfl_oo  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      27/30      5.73G      1.358      1.304      1.401      1.463      1.638      1.352          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:37<00:00,  1.02it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.07it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        200        540      0.373      0.281      0.119     0.0522\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem     box_om     cls_om     dfl_om     box_oo     cls_oo     dfl_oo  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      28/30      5.97G      1.343      1.262      1.362      1.446       1.59      1.302          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:33<00:00,  1.14it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:05<00:00,  1.33s/it]\n"]},{"output_type":"stream","name":"stdout","text":["                   all        200        540      0.389      0.191      0.126     0.0758\n","\n","      Epoch    GPU_mem     box_om     cls_om     dfl_om     box_oo     cls_oo     dfl_oo  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      29/30      5.84G      1.314      1.219      1.361      1.449       1.54      1.321          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:33<00:00,  1.15it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.30it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        200        540       0.71      0.197      0.157     0.0932\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem     box_om     cls_om     dfl_om     box_oo     cls_oo     dfl_oo  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      30/30      6.06G      1.303      1.236      1.377      1.398      1.535       1.33          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38/38 [00:37<00:00,  1.02it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.25it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        200        540      0.687       0.23      0.184      0.102\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","30 epochs completed in 0.385 hours.\n","Optimizer stripped from /content/drive/MyDrive/runs/OpenImageTest/OpenImageTest-20240825-12:17/weights/last.pt, 5.8MB\n","Optimizer stripped from /content/drive/MyDrive/runs/OpenImageTest/OpenImageTest-20240825-12:17/weights/best.pt, 5.8MB\n","\n","Validating /content/drive/MyDrive/runs/OpenImageTest/OpenImageTest-20240825-12:17/weights/best.pt...\n","Ultralytics YOLOv8.1.34 ðŸš€ Python-3.10.12 torch-2.3.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","YOLOv10n summary (fused): 285 layers, 2695976 parameters, 0 gradients, 8.2 GFLOPs\n"]},{"output_type":"stream","name":"stderr","text":["                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:02<00:02,  1.10s/it]"]}],"source":["%cd {HOME}\n","\n","from datetime import datetime\n","from ultralytics import YOLOv10\n","\n","#Session Variables\n","T_STAMP         = datetime.now().strftime('%Y%m%d-%H:%M')\n","RUN_NAME        = SESSION_NAME + '-' + T_STAMP\n","\n","#Create Output Path\n","RUNS_PATH       = os.path.join(HOME, 'runs')\n","SESSION_PATH    = os.path.join(RUNS_PATH, SESSION_NAME)\n","OUTPUT_PATH     = os.path.join(SESSION_PATH, RUN_NAME)\n","\n","#Create YOLOv10 model\n","model           = YOLOv10(MODEL_PATH)\n","\n","#Train the model\n","train = model.train(\n","        #Train Settings\n","        model=MODEL_PATH, \t          #Specifies the model file for training.\n","        data=DATASET_YAML_PATH,       #Path to the dataset configuration file (e.g., coco8.yaml).\n","        epochs=EPOCHS, \t                  #Total number of training epochs.\n","        time=None, \t                  #Maximum training time in hours.\n","        patience=100, \t              #Number of epochs to wait without improvement in validation metrics before early stopping the training.\n","        batch=-1, \t                  #Batch size, with three modes: set as an integer (e.g., batch=16), auto mode for 60% GPU memory utilization (batch=-1), or auto mode with specified utilization fraction (batch=0.70).\n","        imgsz=640, \t                  #Target image size for training.\n","\n","        save=True, \t                  #Enables saving of training checkpoints and final model weights.\n","        save_period=-1, \t            #Frequency of saving model checkpoints, specified in epochs.\n","        cache=False, \t                #Enables caching of dataset images in memory (True/ram), on disk (disk), or disables it (False).\n","        device=None, \t                  #Specifies the computational device(s) for training\n","        workers=8, \t                  #Number of worker threads for data loading (per RANK if Multi-GPU training).\n","        project=SESSION_PATH, \t      #Name of the project directory where training outputs are saved.\n","        name=RUN_NAME, \t              #Name of the training run.\n","        exist_ok=False, \t            #If True, allows overwriting of an existing project/name directory.\n","        pretrained=True,\t            #Determines whether to start training from a pretrained model.\n","\n","        optimizer='auto', \t          #Choice of optimizer for training.\n","        verbose=False, \t              #Enables verbose output during training, providing detailed logs and progress updates.\n","        seed=0, \t                    #Sets the random seed for training, ensuring reproducibility of results across runs with the same configurations.\n","        deterministic=True, \t        #Forces deterministic algorithm use, ensuring reproducibility but may affect performance and speed due to the restriction on non-deterministic algorithms.\n","        single_cls=False, \t          #Treats all classes in multi-class datasets as a single class during training.\n","        rect=False, \t                #Enables rectangular training, optimizing batch composition for minimal padding. Can improve efficiency and speed but may affect model accuracy.\n","        cos_lr=False,\t                #Utilizes a cosine learning rate scheduler, adjusting the learning rate following a cosine curve over epochs. Helps in managing learning rate for better convergence.\n","        close_mosaic=10, \t            #Disables mosaic data augmentation in the last N epochs to stabilize training before completion. Setting to 0 disables this feature.\n","        resume=False, \t              #Resumes training from the last saved checkpoint. Automatically loads model weights, optimizer state, and epoch count, continuing training seamlessly.\n","        amp=True, \t                  #Enables Automatic Mixed Precision (AMP) training, reducing memory usage and possibly speeding up training with minimal impact on accuracy.\n","        fraction=1.0, \t              #Specifies the fraction of the dataset to use for training. Allows for training on a subset of the full dataset, useful for experiments or when resources are limited.\n","        profile=False, \t              #Enables profiling of ONNX and TensorRT speeds during training, useful for optimizing model deployment.\n","\n","        freeze=None,          \t      #Freezes the first N layers of the model or specified layers by index, reducing the number of trainable parameters. Useful for fine-tuning or transfer learning.\n","        lr0=0.01, \t                  #Initial learning rate (i.e. SGD=1E-2, Adam=1E-3) . Adjusting this value is crucial for the optimization process, influencing how rapidly model weights are updated.\n","        lrf=0.1, \t                    #Final learning rate as a fraction of the initial rate = (lr0 * lrf), used in conjunction with schedulers to adjust the learning rate over time.\n","        momentum=0.937, \t            #Momentum factor for SGD or beta1 for Adam optimizers, influencing the incorporation of past gradients in the current update.\n","        weight_decay=0.0005, \t        #L2 regularization term, penalizing large weights to prevent overfitting.\n","        warmup_epochs=3.0, \t          #Number of epochs for learning rate warmup, gradually increasing the learning rate from a low value to the initial learning rate to stabilize training early on.\n","        warmup_momentum=0.8, \t        #Initial momentum for warmup phase, gradually adjusting to the set momentum over the warmup period.\n","        warmup_bias_lr=0.1, \t        #Learning rate for bias parameters during the warmup phase, helping stabilize model training in the initial epochs.\n","\n","        box=7.5, \t                    #Weight of the box loss component in the loss function, influencing how much emphasis is placed on accurately predicting bounding box coordinates.\n","        cls=0.5, \t                    #Weight of the classification loss in the total loss function, affecting the importance of correct class prediction relative to other components.\n","        dfl=1.5, \t                    #Weight of the distribution focal loss, used in certain YOLO versions for fine-grained classification.\n","        pose=12.0, \t                  #Weight of the pose loss in models trained for pose estimation, influencing the emphasis on accurately predicting pose keypoints.\n","        kobj=2.0, \t                  #Weight of the keypoint objectness loss in pose estimation models, balancing detection confidence with pose accuracy.\n","        label_smoothing=0.0, \t        #Applies label smoothing, softening hard labels to a mix of the target label and a uniform distribution over labels, can improve generalization.\n","        nbs=64, \t                    #Nominal batch size for normalization of loss.\n","\n","        overlap_mask=True, \t          #Determines whether segmentation masks should overlap during training, applicable in instance segmentation tasks.\n","        mask_ratio=4, \t              #Downsample ratio for segmentation masks, affecting the resolution of masks used during training.\n","        dropout=0.0, \t                #Dropout rate for regularization in classification tasks, preventing overfitting by randomly omitting units during training.\n","        val=True, \t                  #Enables validation during training, allowing for periodic evaluation of model performance on a separate dataset.\n","        plots=True,\t                  #Generates and saves plots of training and validation metrics, as well as prediction examples, providing visual insights into model performance and learning progression.\n","\n","        #Augmentation Arguments\n","        hsv_h=0.015,                  #Adjusts the hue of the image by a fraction of the color wheel, introducing color variability. Helps the model generalize across different lighting conditions.\n","        hsv_s=0.7,                    #Alters the saturation of the image by a fraction, affecting the intensity of colors. Useful for simulating different environmental conditions.\n","        hsv_v=0.4,                    #Modifies the value (brightness) of the image by a fraction, helping the model to perform well under various lighting conditions.\n","        degrees=0.0,                  #Rotates the image randomly within the specified degree range, improving the model's ability to recognize objects at various orientations.\n","        translate=0.1,                #Translates the image horizontally and vertically by a fraction of the image size, aiding in learning to detect partially visible objects.\n","\n","        scale=0.5,                    #Scales the image by a gain factor, simulating objects at different distances from the camera.\n","        shear=0.0,                    #Shears the image by a specified degree, mimicking the effect of objects being viewed from different angles.\n","        perspective=0.0,              #Applies a random perspective transformation to the image, enhancing the model's ability to understand objects in 3D space.\n","        flipud=0.0,                   #Flips the image upside down with the specified probability, increasing the data variability without affecting the object's characteristics.\n","        fliplr=0.5,                   #Flips the image left to right with the specified probability, useful for learning symmetrical objects and increasing dataset diversity.\n","\n","        bgr=0.0, \t                    #Flips the image channels from RGB to BGR with the specified probability, useful for increasing robustness to incorrect channel ordering.\n","        mosaic=1.0, \t                #Combines four training images into one, simulating different scene compositions and object interactions. Highly effective for complex scene understanding.\n","        mixup=0.0, \t                  #Blends two images and their labels, creating a composite image. Enhances the model's ability to generalize by introducing label noise and visual variability.\n","        copy_paste=0.0,\t              #Copies objects from one image and pastes them onto another, useful for increasing object instances and learning object occlusion.\n","        auto_augment='randaugment',   #Automatically applies a predefined augmentation policy (randaugment, autoaugment, augmix), optimizing for classification tasks by diversifying the visual features.\n","        erasing=0.4,                  #Randomly erases a portion of the image during classification training, encouraging the model to focus on less obvious features for recognition.\n","        crop_fraction=1.0,            #Crops the classification image to a fraction of its size to emphasize central features and adapt to object scales, reducing background distractions.\n",")"]},{"cell_type":"markdown","metadata":{"id":"Y14cj1IFmPhR"},"source":["# Display Training Information"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"_J35i8Ofhjxa","colab":{"base_uri":"https://localhost:8080/","height":248},"executionInfo":{"status":"error","timestamp":1724589888192,"user_tz":-120,"elapsed":581,"user":{"displayName":"Morne Le Roux","userId":"03856785595107178262"}},"outputId":"00f561aa-9e85-4a2b-8fc3-52ccd3498f23"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: '{HOME}'\n","/content\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'OUTPUT_PATH' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-422067bdcedf>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#Display Confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'{OUTPUT_PATH}/confusion_matrix.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'{OUTPUT_PATH}/results.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'OUTPUT_PATH' is not defined"]}],"source":["%cd {HOME}\n","\n","#Display Confusion matrix\n","from IPython.display import Image\n","Image(filename=f'{OUTPUT_PATH}/confusion_matrix.png', width=600)\n","Image(filename=f'{OUTPUT_PATH}/results.png', width=600)"]},{"cell_type":"markdown","metadata":{"id":"GEaCTVL4oB4h"},"source":["# Compare Pre-trained and Trained Model"]},{"cell_type":"markdown","metadata":{"id":"s5GFPofEcb1-"},"source":["## Prepare Dataset for validation\n","We need to have a version of the matches the coco dataset on which the pre-trained model was trained on. For example: The custom dataset has one class of 'sheep' at index '0'. But the pre-trained model was trained on 80 classes where 'person' has index '0', and sheep has index '18'. With the validation dataset, the `/labels/` directory has the actual class stored.\n","\n","The pretrained model will go through the imageset. It will detect an object of index '18'. The (unedited) validation set will say the object belongs to class of index '0' and therefore an inaccurate detection. Thus, the original dataset needs to be converted the the coco class index. All the sheep instances of index '0', should now be changed to index '18'."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-QzmjpOVgFrY"},"outputs":[],"source":["import locale\n","locale.getpreferredencoding = lambda: \"UTF-8\"\n","\n","#Create Dataset folder\n","COCO_DATASET_DIR = '/content/COCO_dataset'\n","COCO_DATASET_YAML_PATH = COCO_DATASET_DIR + '/dataset.yaml'\n","print(COCO_DATASET_YAML_PATH)\n","print(DATASET_DIR)\n","#Create a copy\n","!rm -rf {COCO_DATASET_DIR}\n","!cp -r {DATASET_DIR} {COCO_DATASET_DIR}\n","\n","#Remove all *.cache files\n","%cd {COCO_DATASET_DIR}\n","!rm -r train/*.cache valid/*.cache test/*.cache\n","%cd {HOME}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"23XsFXvqxozI"},"outputs":[],"source":["#Create new YAMl file to describe COCO classes for pre-trained model\n","COCO_CLASS_list = [\n","    \"0: person\",\n","    \"1: bicycle\",\n","    \"2: car\",\n","    \"3: motorcycle\",\n","    \"4: airplane\",\n","    \"5: bus\",\n","    \"6: train\",\n","    \"7: truck\",\n","    \"8: boat\",\n","    \"9: traffic light\",\n","    \"10: fire hydrant\",\n","    \"11: stop sign\",\n","    \"12: parking meter\",\n","    \"13: bench\",\n","    \"14: bird\",\n","    \"15: cat\",\n","    \"16: dog\",\n","    \"17: horse\",\n","    \"18: sheep\",\n","    \"19: cow\",\n","    \"20: elephant\",\n","    \"21: bear\",\n","    \"22: zebra\",\n","    \"23: giraffe\",\n","    \"24: backpack\",\n","    \"25: umbrella\",\n","    \"26: handbag\",\n","    \"27: tie\",\n","    \"28: suitcase\",\n","    \"29: frisbee\",\n","    \"30: skis\",\n","    \"31: snowboard\",\n","    \"32: sports ball\",\n","    \"33: kite\",\n","    \"34: baseball bat\",\n","    \"35: baseball glove\",\n","    \"36: skateboard\",\n","    \"37: surfboard\",\n","    \"38: tennis racket\",\n","    \"39: bottle\",\n","    \"40: wine glass\",\n","    \"41: cup\",\n","    \"42: fork\",\n","    \"43: knife\",\n","    \"44: spoon\",\n","    \"45: bowl\",\n","    \"46: banana\",\n","    \"47: apple\",\n","    \"48: sandwich\",\n","    \"49: orange\",\n","    \"50: broccoli\",\n","    \"51: carrot\",\n","    \"52: hot dog\",\n","    \"53: pizza\",\n","    \"54: donut\",\n","    \"55: cake\",\n","    \"56: chair\",\n","    \"57: couch\",\n","    \"58: potted plant\",\n","    \"59: bed\",\n","    \"60: dining table\",\n","    \"61: toilet\",\n","    \"62: TV\",\n","    \"63: laptop\",\n","    \"64: mouse\",\n","    \"65: remote\",\n","    \"66: keyboard\",\n","    \"67: cell phone\",\n","    \"68: microwave\",\n","    \"69: oven\",\n","    \"70: toaster\",\n","    \"71: sink\",\n","    \"72: refrigerator\",\n","    \"73: book\",\n","    \"74: clock\",\n","    \"75: vase\",\n","    \"76: scissors\",\n","    \"77: teddy bear\",\n","    \"78: hair drier\",\n","    \"79: toothbrush\"\n","]\n","\n","#Remove previous YAML file\n","!rm {COCO_DATASET_YAML_PATH}\n","\n","#Write new YAML file with COCO classes\n","with open(COCO_DATASET_YAML_PATH, 'a') as file:\n","    file.write(\"names:\\n\")\n","    for item in COCO_CLASS_list:\n","        file.write(f\"  {item}\\n\")\n","    file.write(\"\\ntest: ../images/test\\ntrain: ../images/train/\\nval: ../images/val/\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r9SAfn9lixzy"},"outputs":[],"source":["# Define the folder containing the label files\n","label_folder = COCO_DATASET_DIR + '/labels/val'\n","\n","# Process each file in the label folder\n","for filename in os.listdir(label_folder):\n","    file_path = os.path.join(label_folder, filename)\n","    with open(file_path, 'r') as file:\n","        lines = file.readlines()\n","\n","    # Replace numbers in each line\n","    new_lines = []\n","    for line in lines:\n","        components = line.strip().split()\n","        if components[0] in number_mapping:\n","              new_value = number_mapping[components[0]]\n","              if new_value == 'remove':\n","                    continue  # Skip this line if marked for removal\n","              else:\n","                    components[0] = number_mapping[components[0]]\n","                    new_line = ' '.join(components)\n","                    new_lines.append(new_line)\n","\n","    # Write the modified contents back to the file\n","    with open(file_path, 'w') as file:\n","        file.write('\\n'.join(new_lines) + '\\n')\n","\n","print('Number replacement complete.')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AY1ajwSzyXCE"},"outputs":[],"source":["%cd {HOME}\n","from ultralytics import YOLOv10\n","\n","#Get model paths\n","TRAINED_MODEL_PATH      = f'{OUTPUT_PATH}/weights/best.pt'\n","\n","\n","#pretrained_model.classes = [18]\n","trained_model    = YOLOv10(TRAINED_MODEL_PATH)\n","\n","#Validate trained model with 'test' data\n","trained_model_metrics = trained_model.val(\n","    data=DATASET_YAML_PATH,     #Specifies the path to the dataset configuration (yaml) file\n","    imgsz=640,                  #Defines the size of input images\n","    batch=16,          \t        #Sets the number of images per batch. Use -1 for AutoBatch, which automatically adjusts based on GPU memory availability.\n","    save_json=True,            #If True, saves the results to a JSON file for further analysis or integration with other tools.\n","    save_hybrid=False,          #If True, saves a hybrid version of labels that combines original annotations with additional model predictions.\n","    conf=0.5, \t                #Sets the minimum confidence threshold for detections. Detections with confidence below this threshold are discarded.\n","    iou=0.6, \t                  #Sets the Intersection Over Union (IoU) threshold for Non-Maximum Suppression (NMS). Helps in reducing duplicate detections.\n","    max_det=50, \t              #Limits the maximum number of detections per image. Useful in dense scenes to prevent excessive detections.\n","    half=True, \t                #Enables half-precision (FP16) computation, reducing memory usage and potentially increasing speed with minimal impact on accuracy.\n","    device=None, \t              #Specifies the device for validation (cpu, cuda:0, etc.). Allows flexibility in utilizing CPU or GPU resources.\n","    dnn=False, \t                #If True, uses the OpenCV DNN module for ONNX model inference, offering an alternative to PyTorch inference methods.\n","    plots=True,\t                #When set to True, generates and saves plots of predictions versus ground truth for visual evaluation of the model's performance.\n","    rect=False, \t              #If True, uses rectangular inference for batching, reducing padding and potentially increasing speed and efficiency.\n","    split='val',               #Determines the dataset split to use for validation (val, test, or train).\n",")\n","\n","#Define Models\n","\n","PRETRAINED_MODEL_PATH   = MODEL_PATH\n","pretrained_model = YOLOv10(PRETRAINED_MODEL_PATH)\n","\n","#Validate trained model with 'test' data\n","pretrained_model_metrics = pretrained_model.val(\n","    data=COCO_DATASET_YAML_PATH,     #Specifies the path to the dataset configuration (yaml) file\n","    imgsz=640,                  #Defines the size of input images\n","    batch=16,          \t        #Sets the number of images per batch. Use -1 for AutoBatch, which automatically adjusts based on GPU memory availability.\n","    save_json=False,            #If True, saves the results to a JSON file for further analysis or integration with other tools.\n","    save_hybrid=True,          #If True, saves a hybrid version of labels that combines original annotations with additional model predictions.\n","    conf=0.5, \t                #Sets the minimum confidence threshold for detections. Detections with confidence below this threshold are discarded.\n","    iou=0.6, \t                  #Sets the Intersection Over Union (IoU) threshold for Non-Maximum Suppression (NMS). Helps in reducing duplicate detections.\n","    max_det=50, \t              #Limits the maximum number of detections per image. Useful in dense scenes to prevent excessive detections.\n","    half=True, \t                #Enables half-precision (FP16) computation, reducing memory usage and potentially increasing speed with minimal impact on accuracy.\n","    device=None, \t              #Specifies the device for validation (cpu, cuda:0, etc.). Allows flexibility in utilizing CPU or GPU resources.\n","    dnn=False, \t                #If True, uses the OpenCV DNN module for ONNX model inference, offering an alternative to PyTorch inference methods.\n","    plots=True,\t                #When set to True, generates and saves plots of predictions versus ground truth for visual evaluation of the model's performance.\n","    rect=False, \t              #If True, uses rectangular inference for batching, reducing padding and potentially increasing speed and efficiency.\n","    split='val',               #Determines the dataset split to use for validation (val, test, or train).\n","    #classes=[17,18,19],         #17 - Horse, 18- Sheep, 19- Cow\n",")"]},{"cell_type":"markdown","metadata":{"id":"ibNL8dwU1Jqw"},"source":["**NOTE:** Let's randomly select an image from our validation set and visualize the results."]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","def plot_metrics_comparison(title,model1_name,model1_metrics,model2_name, model2_metrics,min,max):\n","    metrics = list(model2_metrics.keys())\n","    model2_values = [model2_metrics[metric] for metric in metrics]\n","    model1_values = []\n","    for metric in metrics :\n","      if str(metric) in model1_metrics:\n","          model1_values.append(model1_metrics[metric])\n","      else:\n","          model1_values.append(0)\n","    #model2_values = [model2_metrics[metric] for metric in metrics]\n","    x = range(len(metrics))\n","\n","    fig, ax = plt.subplots(figsize=(10, 6))\n","    bar_width = 0.35\n","\n","    bars1 = ax.bar(x, model1_values, width=bar_width, label=model1_name, align='center')\n","    bars2 = ax.bar([p + bar_width for p in x], model2_values, width=bar_width, label=model2_name, align='center')\n","\n","    # Add text annotations for Model 1\n","    for bar in bars1:\n","        yval = bar.get_height()\n","        ax.text(bar.get_x() + bar.get_width() / 2, yval + 0.01, round(yval, 2), ha='center', va='bottom')\n","\n","    # Add text annotations for Model 2\n","    for bar in bars2:\n","        yval = bar.get_height()\n","        ax.text(bar.get_x() + bar.get_width() / 2, yval + 0.01, round(yval, 2), ha='center', va='bottom')\n","\n","    ax.set_xlabel('Metrics')\n","    ax.set_ylabel('Values')\n","    ax.set_title(title)\n","    ax.set_xticks([p + bar_width / 2 for p in x])\n","    ax.set_xticklabels(metrics)\n","    ax.set_ylim(min, max)  # All metrics values range between 0 and 1\n","    ax.legend()\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","#Pretrained Model Metric Allocation\n","PT_box = pretrained_model_metrics.box\n","PT_box_metrics = {\n","    'Precision'   : PT_box.mp,\n","    'Recall'      : PT_box.mr,\n","    'mAP50'       : PT_box.map50,\n","    'mAP75'       : PT_box.map75,\n","    'mAP50-95'    : PT_box.map,\n","    'Fitness'     : PT_box.fitness(),\n","}\n","\n","PT_speed = pretrained_model_metrics.speed\n","PT_speed_metrics ={\n","    'Preprocess'  : PT_speed['preprocess'],\n","    'Inference'   : PT_speed['inference'],\n","    'Postprocess' : PT_speed['postprocess'],\n","}\n","\n","#Trained Model Metric Allocation\n","TR_box = trained_model_metrics.box\n","TR_speed = trained_model_metrics.speed\n","TR_box_metrics = {\n","    'Precision'   : TR_box.mp,\n","    'Recall'      : TR_box.mr,\n","    'mAP50'       : TR_box.map50,\n","    'mAP75'       : TR_box.map75,\n","    'mAP50-95'    : TR_box.map,\n","    'Fitness'     : TR_box.fitness(),\n","}\n","TR_speed_metrics ={\n","    'Preprocess'  : TR_speed['preprocess'],\n","    'Inference'   : TR_speed['inference'],\n","    'Postprocess' : TR_speed['postprocess'],\n","}\n","\n","plot_metrics_comparison('Model Performance' ,'Pre-trained Model',PT_box_metrics,'Custom Model',TR_box_metrics,0,1.05)\n","plot_metrics_comparison('Runtime Performance','Pre-trained Model',PT_speed_metrics,'Custom Model',TR_speed_metrics,0,50)"],"metadata":{"id":"cVfnLHxgs7mE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ResultSummary:\n","    # Constructor to initialize the object\n","    def __init__(self,val):\n","        self.Precision = {}\n","        self.Recall = {}\n","        self.F1 = {}\n","        self.mAP50 = {}\n","        self.mAP5095 = {}\n","\n","        for i, c in enumerate(val.box.ap_class_index):\n","            self.Precision[val.names[c].capitalize()]    = val.box.p[i]      #val.box.class_result(i)[0]\n","            self.Recall[val.names[c].capitalize()]       = val.box.r[i]      #val.box.class_result(i)[1]\n","            self.F1[val.names[c].capitalize()]           = val.box.f1[i]\n","            self.mAP50[val.names[c].capitalize()]        = val.box.ap50[i]   #val.box.class_result(i)[2]\n","            self.mAP5095[val.names[c].capitalize()]      = val.box.ap[i]     #val.box.class_result(i)[3]\n","\n","PT_result = ResultSummary(pretrained_model_metrics)\n","TR_result = ResultSummary(trained_model_metrics)\n","\n","#Per Class Metrics\n","plot_metrics_comparison('Precision','Pre-trained Model',PT_result.Precision,'Trained Model',TR_result.Precision,0,1)\n","plot_metrics_comparison('Recall','Pre-trained Model',PT_result.Recall,'Trained Model',TR_result.Recall,0,1)\n","plot_metrics_comparison('F1','Pre-trained Model',PT_result.F1,'Trained Model',TR_result.F1,0,1)\n","plot_metrics_comparison('mAP50','Pre-trained Model',PT_result.mAP50,'Trained Model',TR_result.mAP50,0,1)\n","plot_metrics_comparison('mAP5095','Pre-trained Model',PT_result.mAP5095,'Trained Model',TR_result.mAP5095,0,1)"],"metadata":{"id":"7LQFeFW95pWd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0doYvk4PMza2"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"tfod","language":"python","name":"tfod"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}